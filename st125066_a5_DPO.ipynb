{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/pdf/2305.18290.pdf)\n",
    "\n",
    "### Reference Code \n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the final dataset object should contain these 3 entries if you use the default DPODataCollatorWithPadding data collator. \n",
    "\n",
    "The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(f\"cuda:{1}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        \"hello\",\n",
    "        \"how are you\",\n",
    "        \"What is your name?\",\n",
    "        \"What is your name?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"hi nice to meet you\",\n",
    "        \"I am fine\",\n",
    "        \"My name is Mary\",\n",
    "        \"My name is Mary\",\n",
    "        \"Python\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        \"leave me alone\",\n",
    "        \"I am not fine\",\n",
    "        \"Whats it to you?\",\n",
    "        \"I dont have a name\",\n",
    "        \"Javascript\",\n",
    "        \"C++\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from trl import DPOTrainer, DPOConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load a pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model_name_or_path = \"gpt2\"\n",
    "    ignore_bias_buffers = False\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    if ignore_bias_buffers:\n",
    "        # torch distributed hack\n",
    "        model._ddp_params_and_buffers_to_ignore = [\n",
    "            name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "        ]\n",
    "    \n",
    "    model_ref = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model,model_ref,tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO trainer expects a model of AutoModelForCausalLM, compared to PPO that expects AutoModelForCausalLMWithValueHead for the value function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Anthropic Helpful-Harmless dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_anthropic_prompt(prompt_and_response):\n",
    "    \"\"\"Extract the anthropic prompt from a prompt and response pair.\"\"\"\n",
    "    search_term = \"\\n\\nAssistant:\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "def get_hh(split: str, sanity_check: bool = False, silent: bool = False, cache_dir: str = None) -> Dataset:\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format.\n",
    "\n",
    "    The dataset is converted to a dictionary with the following structure:\n",
    "    {\n",
    "        'prompt': List[str],\n",
    "        'chosen': List[str],\n",
    "        'rejected': List[str],\n",
    "    }\n",
    "\n",
    "    Prompts should be structured as follows:\n",
    "      \\n\\nHuman: <prompt>\\n\\nAssistant:\n",
    "    Multiple turns are allowed, but the prompt should always start with \\n\\nHuman: and end with \\n\\nAssistant:.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = load_dataset(\"Anthropic/hh-rlhf\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
    "\n",
    "    def split_prompt_and_responses(sample) -> Dict[str, str]:\n",
    "        prompt = extract_anthropic_prompt(sample[\"chosen\"])\n",
    "        return {\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": sample[\"chosen\"][len(prompt) :],\n",
    "            \"rejected\": sample[\"rejected\"][len(prompt) :],\n",
    "        }\n",
    "\n",
    "    return dataset.map(split_prompt_and_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sanity_check = True\n",
    "train_dataset = get_hh(\"train\", sanity_check=sanity_check)\n",
    "eval_dataset = get_hh(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. initialize training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "max_length= 512 \n",
    "max_prompt_length = 128 \n",
    "max_target_length =128 \n",
    "label_pad_token_id = 100\n",
    "max_steps = 1000\n",
    "# instrumentation\n",
    "sanity_check = True\n",
    "report_to = None\n",
    "gradient_checkpointing = None\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st125066/.local/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = DPOConfig(\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    max_steps=max_steps,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=5,\n",
    "    eval_steps=500,\n",
    "    output_dir=\"./test\",\n",
    "    optim=\"rmsprop\",\n",
    "    warmup_steps=150,\n",
    "    report_to=report_to,\n",
    "   # bf16=True,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_length=max_length,\n",
    "    max_completion_length=max_target_length,\n",
    "    beta=beta,  # Add the beta parameter here\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. initialize the DPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dpo_trainer():\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,  # Changed from processing_class to tokenizer\n",
    "    )\n",
    "\n",
    "    return dpo_trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will experiment with three hyperparameters as follows:\n",
    "- learning_rate (0.001, 0.01, 0.1)\n",
    "- per_device_train_batch_size (2,4,8)\n",
    "- beta (0.1,0.2,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3876042/3230848797.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "per_device_train_batch_size = 8\n",
    "beta = 0.1\n",
    "output_dir=\"./test0\"\n",
    "\n",
    "training_args.learning_rate = learning_rate\n",
    "training_args.per_device_train_batch_size = per_device_train_batch_size\n",
    "training_args.beta = beta\n",
    "training_args.output_dir = output_dir\n",
    "\n",
    "model,model_ref,tokenizer =  get_model()\n",
    "\n",
    "dpo_trainer = get_dpo_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 09:39, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>2.946879</td>\n",
       "      <td>-16.692886</td>\n",
       "      <td>-19.559902</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>2.867015</td>\n",
       "      <td>-283.705658</td>\n",
       "      <td>-334.188507</td>\n",
       "      <td>-30.103504</td>\n",
       "      <td>-27.820219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.476468</td>\n",
       "      <td>-30.020247</td>\n",
       "      <td>-34.729416</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>4.709165</td>\n",
       "      <td>-416.979248</td>\n",
       "      <td>-485.883606</td>\n",
       "      <td>-71.702415</td>\n",
       "      <td>-67.645248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.01318841723894, metrics={'train_runtime': 580.053, 'train_samples_per_second': 13.792, 'train_steps_per_second': 1.724, 'total_flos': 0.0, 'train_loss': 1.01318841723894, 'epoch': 8.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model('./model0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3876042/3230848797.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "per_device_train_batch_size = 4\n",
    "beta = 0.2\n",
    "output_dir=\"./test1\"\n",
    "\n",
    "training_args.learning_rate = learning_rate\n",
    "training_args.per_device_train_batch_size = per_device_train_batch_size\n",
    "training_args.beta = beta\n",
    "training_args.output_dir = output_dir\n",
    "\n",
    "model,model_ref,tokenizer =  get_model()\n",
    "\n",
    "dpo_trainer = get_dpo_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>7.611900</td>\n",
       "      <td>10.232700</td>\n",
       "      <td>-36.708580</td>\n",
       "      <td>-45.741634</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>9.033051</td>\n",
       "      <td>-300.319702</td>\n",
       "      <td>-367.297668</td>\n",
       "      <td>-10.869419</td>\n",
       "      <td>-10.867895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.894300</td>\n",
       "      <td>9.678289</td>\n",
       "      <td>-40.634819</td>\n",
       "      <td>-49.387379</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>8.752555</td>\n",
       "      <td>-319.950897</td>\n",
       "      <td>-385.526367</td>\n",
       "      <td>-11.941029</td>\n",
       "      <td>-11.876305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=7.809128794193268, metrics={'train_runtime': 413.3771, 'train_samples_per_second': 9.676, 'train_steps_per_second': 2.419, 'total_flos': 0.0, 'train_loss': 7.809128794193268, 'epoch': 4.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model('./model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3876042/3230848797.py:2: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "per_device_train_batch_size = 2\n",
    "beta = 0.3\n",
    "output_dir=\"./test2\"\n",
    "\n",
    "training_args.learning_rate = learning_rate\n",
    "training_args.per_device_train_batch_size = per_device_train_batch_size\n",
    "training_args.beta = beta\n",
    "training_args.output_dir = output_dir\n",
    "\n",
    "model,model_ref,tokenizer =  get_model()\n",
    "\n",
    "dpo_trainer = get_dpo_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>203.088500</td>\n",
       "      <td>155.744827</td>\n",
       "      <td>-583.949890</td>\n",
       "      <td>-697.817749</td>\n",
       "      <td>0.553000</td>\n",
       "      <td>113.867889</td>\n",
       "      <td>-2063.276367</td>\n",
       "      <td>-2464.648438</td>\n",
       "      <td>-3.729004</td>\n",
       "      <td>-3.728814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.723000</td>\n",
       "      <td>15.010561</td>\n",
       "      <td>-57.645195</td>\n",
       "      <td>-70.634834</td>\n",
       "      <td>0.568000</td>\n",
       "      <td>12.989642</td>\n",
       "      <td>-308.927399</td>\n",
       "      <td>-374.038910</td>\n",
       "      <td>-1.612762</td>\n",
       "      <td>-1.613330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=134.4879447426796, metrics={'train_runtime': 190.6906, 'train_samples_per_second': 10.488, 'train_steps_per_second': 5.244, 'total_flos': 0.0, 'train_loss': 134.4879447426796, 'epoch': 2.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model('./model2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Experimental Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Training Loss at 1000 steps</th>\n",
       "      <th>Validation Loss at 1000 steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.476468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.8943</td>\n",
       "      <td>9.678289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.7230</td>\n",
       "      <td>15.010561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate  Batch Size  Beta  Training Loss at 1000 steps  \\\n",
       "0          0.001           8   0.1                       0.0000   \n",
       "1          0.010           4   0.2                       6.8943   \n",
       "2          0.100           2   0.3                       3.7230   \n",
       "\n",
       "   Validation Loss at 1000 steps  \n",
       "0                       4.476468  \n",
       "1                       9.678289  \n",
       "2                      15.010561  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({\"Learning Rate\" : [0.001, 0.01, 0.1],\n",
    "             \"Batch Size\" : [8,4,2],\n",
    "             \"Beta\" : [0.1,0.2,0.3],\n",
    "              \"Training Loss at 1000 steps\" : [0, 6.894300,3.723000],\n",
    "               \"Validation Loss at 1000 steps\" : [4.476468, 9.678289,15.010561]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Upload Model to Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = AutoModelForCausalLM.from_pretrained(\"./model0.pth\")\n",
    "tokenizer0 = AutoTokenizer.from_pretrained(\"./model0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd128c97d2104d8b9db90ca4c12da54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/kaung-nyo-lwin/dpo_gpt2_nlp_a5/commit/c47f92bbfdbc659dcedd6251e6b508a28a7c49df', commit_message='Upload tokenizer', commit_description='', oid='c47f92bbfdbc659dcedd6251e6b508a28a7c49df', pr_url=None, repo_url=RepoUrl('https://huggingface.co/kaung-nyo-lwin/dpo_gpt2_nlp_a5', endpoint='https://huggingface.co', repo_type='model', repo_id='kaung-nyo-lwin/dpo_gpt2_nlp_a5'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.push_to_hub('kaung-nyo-lwin/dpo_gpt2_nlp_a5')\n",
    "tokenizer0.push_to_hub('kaung-nyo-lwin/dpo_gpt2_nlp_a5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limitation of compute resources, the model is trained on a subset of the dataset. Therefore, the model may not perform well on new data. I have tested three hyperparameters and the model with the best performance is trained on. Since increasing batch size is limited by gpu memory, I have experimented to see the decline of training performance with decreasing batch size, increasing learning rate and beta value. According to experimental results, the training performance decline is sigificant when decreasing batch size, increasing learning rate and beta value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
